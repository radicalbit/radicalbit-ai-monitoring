services:
  ui:
    profiles: ["ui"]
    build:
      context: ./ui
      target: dev
    command: yarn start:local
    develop:
      watch:
        - path: ./ui/src
          action: rebuild
    depends_on:
      api:
        condition: service_healthy
    ports:
      - 5173:5173

  api:
    build: ./api
    ports:
      - 9000:9000
    develop:
      watch:
        - path: ./api/app
          action: rebuild
    environment:
      DB_HOST: "postgres"
      DB_PORT: 5432
      DB_USER: "postgres"
      DB_PWD: "postgres"
      DB_NAME: "radicalbit"
      DB_SCHEMA: "public"
      DB_HOST_CH: "clickhouse"
      DB_PORT_CH: 9000
      DB_USER_CH: "default"
      DB_PWD_CH: "default"
      DB_NAME_CH: "default"
      DB_SCHEMA_CH: "default"
      AWS_ACCESS_KEY_ID: "minio"
      AWS_SECRET_ACCESS_KEY: "minio123"
      AWS_REGION: "us-east-1"
      S3_ENDPOINT_URL: "http://minio:9000"
      S3_BUCKET_NAME: "test-bucket"
      KUBECONFIG_FILE_PATH: "/opt/kubeconfig/kubeconfig.yaml"
      SPARK_IMAGE: "radicalbit-spark-py:develop"
    depends_on:
      postgres:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      migrations:
        condition: service_completed_successfully
      createbuckets:
        condition: service_completed_successfully
      k3s:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -s http://localhost:9000/healthcheck" ]
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - ./docker/k3s_data/kubeconfig/kubeconfig.yaml:/opt/kubeconfig/kubeconfig.yaml

  migrations:
    container_name: migrations
    build:
      context: ./api
      dockerfile: ./migrations.Dockerfile
    environment:
      DB_HOST: "postgres"
      DB_PORT: 5432
      DB_USER: "postgres"
      DB_PWD: "postgres"
      DB_NAME: "radicalbit"
      DB_SCHEMA: "public"
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./api/alembic:/app/alembic
      - ./api/app/db/tables:/app/app/db/tables

  postgres:
    image: postgres:latest
    container_name: postgres-container
    ports:
      - 5432:5432
    environment:
      POSTGRES_DB: radicalbit
      POSTGRES_PASSWORD: postgres
    volumes:
      - radicalbit-data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 5s
      timeout: 5s
      retries: 5

  adminer:
    image: adminer:latest
    ports:
      - 8090:8080
    depends_on:
      postgres:
        condition: service_healthy

  minio:
    image: minio/minio
    ports:
      - "9090:9000"
      - "9091:9001"
    volumes:
      - minio_storage:/data
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server --console-address ":9001" /data
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1
      start_period: 5s
      interval: 10s
      timeout: 5s
      retries: 2

  createbuckets:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 minio minio123;
      /usr/bin/mc mb myminio/test-bucket;
      /usr/bin/mc policy set public myminio/test-bucket;
      exit 0;
      "

  init-data:
    profiles: [ "init-data" ]
    image: postgres:latest
    depends_on:
      migrations:
        condition: service_completed_successfully
    environment:
      POSTGRES_DB: radicalbit
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_SCHEMA: public
    volumes:
      - ./init-data/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    entrypoint: >
      /bin/sh -c "
      sleep 10;
      PGPASSWORD=postgres PGOPTIONS="--search_path=$${POSTGRES_SCHEMA}" psql -h postgres -U postgres -d radicalbit -f /docker-entrypoint-initdb.d/init_db.sql;
      "

  minio-mirror:
    profiles: ["init-data"]
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /bin/sh /scripts/mirror_files.sh
    volumes:
      - ./init-data/mirror_files.sh:/scripts/mirror_files.sh
      - ./docs/quickstarts/data:/data

  k3s:
    image: rancher/k3s:v1.30.1-k3s1
    networks:
      - default
    entrypoint: /bin/sh
    command: "-c /opt/entrypoint/entrypoint.sh"
    tmpfs:
      - /run
      - /var/run
    ulimits:
      nproc: 65535
      nofile:
        soft: 65535
        hard: 65535
    privileged: true
    restart: no
    environment:
      K3S_KUBECONFIG_OUTPUT: /output/kubeconfig.yaml
      K3S_KUBECONFIG_MODE: 666
    depends_on:
      docker-client:
        condition: service_completed_successfully
    volumes:
      - k3s-server:/var/lib/rancher/k3s
      # This is just so that we get the kubeconfig file out
      - ./docker/k3s_data/kubeconfig:/output
      # Init spark needed resources, add other custom yaml file here below if needed (e.g. secrets)
      - ./docker/k3s_data/manifests/spark-init.yaml:/var/lib/rancher/k3s/server/manifests/spark-init.yaml
      # Mount entrypoint
      - ./docker/k3s_data/init/entrypoint.sh:/opt/entrypoint/entrypoint.sh
      # Preload docker images
      - ./docker/k3s_data/images:/var/lib/rancher/k3s/agent/images
    expose:
      - "6443"  # Kubernetes API Server
      - "80"    # Ingress controller port 80
      - "443"   # Ingress controller port 443
    ports:
      - 6443:6443
    healthcheck:
      test: [ "CMD-SHELL", "netstat -tuln | grep ':6443 ' || exit 1" ]
      interval: 10s
      timeout: 5s
      start_period: 5s
      retries: 2

  spark-test:
    profiles: ["spark-test"]
    build: ./spark-test
    environment:
      JOB_NAME: "current"
      KUBECONFIG_FILE_PATH: "/opt/kubeconfig/kubeconfig.yaml"
      SPARK_IMAGE: "radicalbit-spark-py:develop"
    volumes:
      - ./docker/k3s_data/kubeconfig/kubeconfig.yaml:/opt/kubeconfig/kubeconfig.yaml

  dind:
    image: docker:dind
    privileged: true
    hostname: dind
    environment:
      DOCKER_TLS_CERTDIR: /certs
    volumes:
      - docker-certs-ca:/certs
      - docker-certs-client:/certs/client
    healthcheck:
      test: nc -w 5 -z localhost 2376
      start_period: 5s
      interval: 10s
      timeout: 5s
      retries: 2

  docker-client:
    image: docker:27.2.1-cli
    command: "/bin/sh -c 'docker build ./spark -t radicalbit-spark-py:develop && docker save radicalbit-spark-py:develop -o /images/radicalbit-spark-py:develop.tar'"
    environment:
      DOCKER_TLS_CERTDIR: /certs
      DOCKER_HOST: tcp://dind:2376
    depends_on:
      dind:
        condition: service_healthy
    volumes:
      - docker-certs-client:/certs/client:ro
      - ./spark:/spark
      - ./docker/k3s_data/images:/images

  k9s:
    profiles: ["k9s"]
    image: quay.io/derailed/k9s:latest
    stdin_open: true
    tty: true
    command: -A
    ports:
      - 4040:4040 # Spark UI port if forwarded from k9s
    volumes:
      - ./docker/k3s_data/kubeconfig/kubeconfig.yaml:/root/.kube/config

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"  # HTTP interface
      - "9002:9000"  # Native client
    healthcheck:
      test: [ "CMD", "clickhouse-client", "--host", "localhost" ]
      interval: 5s
      retries: 5
      start_period: 5s
      timeout: 5s
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=default
    volumes:
      - clickhouse_data:/var/lib/clickhouse

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: otel-collector
    restart: always
    command: [ "--config", "/etc/otel-collector-config.yaml" ]
    volumes:
      - ./otel-collector-config/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"
      - "4318:4318"
    depends_on:
      clickhouse:
        condition: service_healthy

  ch-ui:
    image: ghcr.io/caioricciuti/ch-ui:latest
    restart: always
    ports:
      - "5521:5521"
    environment:
      VITE_CLICKHOUSE_URL: "http://localhost:8123"
      VITE_CLICKHOUSE_USER: "default"
      VITE_CLICKHOUSE_PASS: "default"
    depends_on:
      clickhouse:
        condition: service_healthy

volumes:
  k3s-server: {}
  radicalbit-data: {}
  minio_storage: {}
  docker-certs-ca: {}
  docker-certs-client: {}
  clickhouse_data: {}

networks:
  default:
    ipam:
      driver: default
      config:
        - subnet: "172.98.0.0/16" # Self-defined subnet on local machine